{"version":3,"sources":["../../../../src/client/components/segment-cache/scheduler.ts"],"sourcesContent":["import type { TreePrefetch } from '../../../server/app-render/collect-segment-data'\nimport {\n  requestRouteCacheEntryFromCache,\n  requestSegmentEntryFromCache,\n  EntryStatus,\n  type FulfilledRouteCacheEntry,\n  type RouteCacheEntry,\n} from './cache'\nimport type { RouteCacheKey } from './cache-key'\n\nconst scheduleMicrotask =\n  typeof queueMicrotask === 'function'\n    ? queueMicrotask\n    : (fn: () => unknown) =>\n        Promise.resolve()\n          .then(fn)\n          .catch((error) =>\n            setTimeout(() => {\n              throw error\n            })\n          )\n\nexport type PrefetchTask = {\n  key: RouteCacheKey\n\n  /**\n   * sortId is an incrementing counter\n   *\n   * Newer prefetches are prioritized over older ones, so that as new links\n   * enter the viewport, they are not starved by older links that are no\n   * longer relevant. In the future, we can add additional prioritization\n   * heuristics, like removing prefetches once a link leaves the viewport.\n   *\n   * The sortId is assigned when the prefetch is initiated, and reassigned if\n   * the same URL is prefetched again (effectively bumping it to the top of\n   * the queue).\n   *\n   * TODO: We can add additional fields here to indicate what kind of prefetch\n   * it is. For example, was it initiated by a link? Or was it an imperative\n   * call? If it was initiated by a link, we can remove it from the queue when\n   * the link leaves the viewport, but if it was an imperative call, then we\n   * should keep it in the queue until it's fulfilled.\n   *\n   * We can also add priority levels. For example, hovering over a link could\n   * increase the priority of its prefetch.\n   */\n  sortId: number\n\n  /**\n   * True if the prefetch is blocked by network data. We remove tasks from the\n   * queue once they are blocked, and add them back when they receive data.\n   *\n   * isBlocked also indicates whether the task is currently in the queue; tasks\n   * are removed from the queue when they are blocked. Use this to avoid\n   * queueing the same task multiple times.\n   */\n  isBlocked: boolean\n\n  /**\n   * The index of the task in the heap's backing array. Used to efficiently\n   * change the priority of a task by re-sifting it, which requires knowing\n   * where it is in the array. This is only used internally by the heap\n   * algorithm. The naive alternative is indexOf every time a task is queued,\n   * which has O(n) complexity.\n   */\n  _heapIndex: number\n}\n\nconst enum PrefetchTaskExitStatus {\n  /**\n   * The task yielded because there are too many requests in progress.\n   */\n  InProgress,\n\n  /**\n   * The task is blocked. It needs more data before it can proceed.\n   *\n   * Currently the only reason this happens is we're still waiting to receive a\n   * route tree from the server, because we can't start prefetching the segments\n   * until we know what to prefetch.\n   */\n  Blocked,\n\n  /**\n   * There's nothing left to prefetch.\n   */\n  Done,\n}\n\nconst taskHeap: Array<PrefetchTask> = []\n\n// This is intentionally low so that when a navigation happens, the browser's\n// internal network queue is not already saturated with prefetch requests.\nconst MAX_CONCURRENT_PREFETCH_REQUESTS = 3\nlet inProgressRequests = 0\n\nlet sortIdCounter = 0\nlet didScheduleMicrotask = false\n\n/**\n * Initiates a prefetch task for the given URL. If a prefetch for the same URL\n * is already in progress, this will bump it to the top of the queue.\n *\n * This is not a user-facing function. By the time this is called, the href is\n * expected to be validated and normalized.\n *\n * @param key The RouteCacheKey to prefetch.\n */\nexport function schedulePrefetchTask(key: RouteCacheKey): void {\n  // Spawn a new prefetch task\n  const task: PrefetchTask = {\n    key,\n    sortId: sortIdCounter++,\n    isBlocked: false,\n    _heapIndex: -1,\n  }\n  heapPush(taskHeap, task)\n\n  // Schedule an async task to process the queue.\n  //\n  // The main reason we process the queue in an async task is for batching.\n  // It's common for a single JS task/event to trigger multiple prefetches.\n  // By deferring to a microtask, we only process the queue once per JS task.\n  // If they have different priorities, it also ensures they are processed in\n  // the optimal order.\n  ensureWorkIsScheduled()\n}\n\nfunction ensureWorkIsScheduled() {\n  if (didScheduleMicrotask || !hasNetworkBandwidth()) {\n    // Either we already scheduled a task to process the queue, or there are\n    // too many concurrent requests in progress. In the latter case, the\n    // queue will resume processing once more bandwidth is available.\n    return\n  }\n  didScheduleMicrotask = true\n  scheduleMicrotask(processQueueInMicrotask)\n}\n\n/**\n * Checks if we've exceeded the maximum number of concurrent prefetch requests,\n * to avoid saturating the browser's internal network queue. This is a\n * cooperative limit â€” prefetch tasks should check this before issuing\n * new requests.\n */\nfunction hasNetworkBandwidth(): boolean {\n  // TODO: Also check if there's an in-progress navigation. We should never\n  // add prefetch requests to the network queue if an actual navigation is\n  // taking place, to ensure there's sufficient bandwidth for render-blocking\n  // data and resources.\n  return inProgressRequests < MAX_CONCURRENT_PREFETCH_REQUESTS\n}\n\n/**\n * Notifies the scheduler of an in-progress prefetch request. This is used to\n * control network bandwidth by limiting the number of concurrent requests.\n *\n * @param promise A promise that resolves when the request has finished.\n */\nexport function trackPrefetchRequestBandwidth(\n  promiseForServerData: Promise<unknown>\n) {\n  inProgressRequests++\n  promiseForServerData.then(\n    onPrefetchRequestCompletion,\n    onPrefetchRequestCompletion\n  )\n}\n\nconst noop = () => {}\n\nexport function spawnPrefetchSubtask(promise: Promise<any>) {\n  // When the scheduler spawns an async task, we don't await its result\n  // directly. Instead, the async task writes its result directly into the\n  // cache, then pings the scheduler to continue.\n  //\n  // This function only exists to prevent warnings about unhandled promises.\n  promise.then(noop, noop)\n}\n\nfunction onPrefetchRequestCompletion(): void {\n  inProgressRequests--\n\n  // Notify the scheduler that we have more bandwidth, and can continue\n  // processing tasks.\n  ensureWorkIsScheduled()\n}\n\n/**\n * Notify the scheduler that we've received new data for an in-progress\n * prefetch. The corresponding task will be added back to the queue (unless the\n * task has been canceled in the meantime).\n */\nexport function pingPrefetchTask(task: PrefetchTask) {\n  // \"Ping\" a prefetch that's already in progress to notify it of new data.\n  if (!task.isBlocked) {\n    // Prefetch is already queued.\n    return\n  }\n  // Unblock the task and requeue it.\n  task.isBlocked = false\n  heapPush(taskHeap, task)\n  ensureWorkIsScheduled()\n}\n\nfunction processQueueInMicrotask() {\n  didScheduleMicrotask = false\n\n  // We aim to minimize how often we read the current time. Since nearly all\n  // functions in the prefetch scheduler are synchronous, we can read the time\n  // once and pass it as an argument wherever it's needed.\n  const now = Date.now()\n\n  // Process the task queue until we run out of network bandwidth.\n  let task = heapPeek(taskHeap)\n  while (task !== null && hasNetworkBandwidth()) {\n    const route = requestRouteCacheEntryFromCache(now, task)\n    const exitStatus = pingRouteTree(now, task, route)\n    switch (exitStatus) {\n      case PrefetchTaskExitStatus.InProgress:\n        // The task yielded because there are too many requests in progress.\n        // Stop processing tasks until we have more bandwidth.\n        return\n      case PrefetchTaskExitStatus.Blocked:\n        // The task is blocked. It needs more data before it can proceed.\n        // Keep the task out of the queue until the server responds.\n        task.isBlocked = true\n\n        // Continue to the next task\n        heapPop(taskHeap)\n        task = heapPeek(taskHeap)\n        continue\n      case PrefetchTaskExitStatus.Done:\n        // The prefetch is complete. Continue to the next task.\n        heapPop(taskHeap)\n        task = heapPeek(taskHeap)\n        continue\n      default: {\n        const _exhaustiveCheck: never = exitStatus\n        return\n      }\n    }\n  }\n}\n\nfunction pingRouteTree(\n  now: number,\n  task: PrefetchTask,\n  route: RouteCacheEntry\n): PrefetchTaskExitStatus {\n  switch (route.status) {\n    case EntryStatus.Pending: {\n      // Still pending. We can't start prefetching the segments until the route\n      // tree has loaded.\n      const blockedTasks = route.blockedTasks\n      if (blockedTasks === null) {\n        route.blockedTasks = new Set([task])\n      } else {\n        blockedTasks.add(task)\n      }\n      return PrefetchTaskExitStatus.Blocked\n    }\n    case EntryStatus.Rejected: {\n      // Route tree failed to load. Treat as a 404.\n      return PrefetchTaskExitStatus.Done\n    }\n    case EntryStatus.Fulfilled: {\n      // Recursively fill in the segment tree.\n      if (!hasNetworkBandwidth()) {\n        // Stop prefetching segments until there's more bandwidth.\n        return PrefetchTaskExitStatus.InProgress\n      }\n      const tree = route.tree\n      requestSegmentEntryFromCache(now, task, route, tree.path, '')\n      return pingSegmentTree(now, task, route, tree)\n    }\n    default: {\n      const _exhaustiveCheck: never = route\n      return PrefetchTaskExitStatus.Done\n    }\n  }\n}\n\nfunction pingSegmentTree(\n  now: number,\n  task: PrefetchTask,\n  route: FulfilledRouteCacheEntry,\n  tree: TreePrefetch\n): PrefetchTaskExitStatus.InProgress | PrefetchTaskExitStatus.Done {\n  if (tree.slots !== null) {\n    // Recursively ping the children.\n    for (const parallelRouteKey in tree.slots) {\n      const childTree = tree.slots[parallelRouteKey]\n      if (!hasNetworkBandwidth()) {\n        // Stop prefetching segments until there's more bandwidth.\n        return PrefetchTaskExitStatus.InProgress\n      } else {\n        const childPath = childTree.path\n        const childToken = childTree.token\n        requestSegmentEntryFromCache(now, task, route, childPath, childToken)\n      }\n      const childExitStatus = pingSegmentTree(now, task, route, childTree)\n      if (childExitStatus === PrefetchTaskExitStatus.InProgress) {\n        // Child yielded without finishing.\n        return PrefetchTaskExitStatus.InProgress\n      }\n    }\n  }\n  // This segment and all its children have finished prefetching.\n  return PrefetchTaskExitStatus.Done\n}\n\n// -----------------------------------------------------------------------------\n// The remainider of the module is a MinHeap implementation. Try not to put any\n// logic below here unless it's related to the heap algorithm. We can extract\n// this to a separate module if/when we need multiple kinds of heaps.\n// -----------------------------------------------------------------------------\n\nfunction compareQueuePriority(a: PrefetchTask, b: PrefetchTask) {\n  // Since the queue is a MinHeap, this should return a positive number if b is\n  // higher priority than a, and a negative number if a is higher priority\n  // than b.\n  //\n  // sortId is an incrementing counter assigned to prefetches. We want to\n  // process the newest prefetches first.\n  return b.sortId - a.sortId\n}\n\nfunction heapPush(heap: Array<PrefetchTask>, node: PrefetchTask): void {\n  const index = heap.length\n  heap.push(node)\n  node._heapIndex = index\n  heapSiftUp(heap, node, index)\n}\n\nfunction heapPeek(heap: Array<PrefetchTask>): PrefetchTask | null {\n  return heap.length === 0 ? null : heap[0]\n}\n\nfunction heapPop(heap: Array<PrefetchTask>): PrefetchTask | null {\n  if (heap.length === 0) {\n    return null\n  }\n  const first = heap[0]\n  first._heapIndex = -1\n  const last = heap.pop() as PrefetchTask\n  if (last !== first) {\n    heap[0] = last\n    last._heapIndex = 0\n    heapSiftDown(heap, last, 0)\n  }\n  return first\n}\n\n// Not currently used, but will be once we add the ability to update a\n// task's priority.\n// function heapSift(heap: Array<PrefetchTask>, node: PrefetchTask) {\n//   const index = node._heapIndex\n//   if (index !== -1) {\n//     const parentIndex = (index - 1) >>> 1\n//     const parent = heap[parentIndex]\n//     if (compareQueuePriority(parent, node) > 0) {\n//       // The parent is larger. Sift up.\n//       heapSiftUp(heap, node, index)\n//     } else {\n//       // The parent is smaller (or equal). Sift down.\n//       heapSiftDown(heap, node, index)\n//     }\n//   }\n// }\n\nfunction heapSiftUp(\n  heap: Array<PrefetchTask>,\n  node: PrefetchTask,\n  i: number\n): void {\n  let index = i\n  while (index > 0) {\n    const parentIndex = (index - 1) >>> 1\n    const parent = heap[parentIndex]\n    if (compareQueuePriority(parent, node) > 0) {\n      // The parent is larger. Swap positions.\n      heap[parentIndex] = node\n      node._heapIndex = parentIndex\n      heap[index] = parent\n      parent._heapIndex = index\n\n      index = parentIndex\n    } else {\n      // The parent is smaller. Exit.\n      return\n    }\n  }\n}\n\nfunction heapSiftDown(\n  heap: Array<PrefetchTask>,\n  node: PrefetchTask,\n  i: number\n): void {\n  let index = i\n  const length = heap.length\n  const halfLength = length >>> 1\n  while (index < halfLength) {\n    const leftIndex = (index + 1) * 2 - 1\n    const left = heap[leftIndex]\n    const rightIndex = leftIndex + 1\n    const right = heap[rightIndex]\n\n    // If the left or right node is smaller, swap with the smaller of those.\n    if (compareQueuePriority(left, node) < 0) {\n      if (rightIndex < length && compareQueuePriority(right, left) < 0) {\n        heap[index] = right\n        right._heapIndex = index\n        heap[rightIndex] = node\n        node._heapIndex = rightIndex\n\n        index = rightIndex\n      } else {\n        heap[index] = left\n        left._heapIndex = index\n        heap[leftIndex] = node\n        node._heapIndex = leftIndex\n\n        index = leftIndex\n      }\n    } else if (rightIndex < length && compareQueuePriority(right, node) < 0) {\n      heap[index] = right\n      right._heapIndex = index\n      heap[rightIndex] = node\n      node._heapIndex = rightIndex\n\n      index = rightIndex\n    } else {\n      // Neither child is smaller. Exit.\n      return\n    }\n  }\n}\n"],"names":["requestRouteCacheEntryFromCache","requestSegmentEntryFromCache","EntryStatus","scheduleMicrotask","queueMicrotask","fn","Promise","resolve","then","catch","error","setTimeout","taskHeap","MAX_CONCURRENT_PREFETCH_REQUESTS","inProgressRequests","sortIdCounter","didScheduleMicrotask","schedulePrefetchTask","key","task","sortId","isBlocked","_heapIndex","heapPush","ensureWorkIsScheduled","hasNetworkBandwidth","processQueueInMicrotask","trackPrefetchRequestBandwidth","promiseForServerData","onPrefetchRequestCompletion","noop","spawnPrefetchSubtask","promise","pingPrefetchTask","now","Date","heapPeek","route","exitStatus","pingRouteTree","heapPop","_exhaustiveCheck","status","Pending","blockedTasks","Set","add","Rejected","Fulfilled","tree","path","pingSegmentTree","slots","parallelRouteKey","childTree","childPath","childToken","token","childExitStatus","compareQueuePriority","a","b","heap","node","index","length","push","heapSiftUp","first","last","pop","heapSiftDown","i","parentIndex","parent","halfLength","leftIndex","left","rightIndex","right"],"mappings":"AACA,SACEA,+BAA+B,EAC/BC,4BAA4B,EAC5BC,WAAW,QAGN,UAAS;AAGhB,MAAMC,oBACJ,OAAOC,mBAAmB,aACtBA,iBACA,CAACC,KACCC,QAAQC,OAAO,GACZC,IAAI,CAACH,IACLI,KAAK,CAAC,CAACC,QACNC,WAAW;YACT,MAAMD;QACR;;AAsEZ,MAAME,WAAgC,EAAE;AAExC,6EAA6E;AAC7E,0EAA0E;AAC1E,MAAMC,mCAAmC;AACzC,IAAIC,qBAAqB;AAEzB,IAAIC,gBAAgB;AACpB,IAAIC,uBAAuB;AAE3B;;;;;;;;CAQC,GACD,OAAO,SAASC,qBAAqBC,GAAkB;IACrD,4BAA4B;IAC5B,MAAMC,OAAqB;QACzBD;QACAE,QAAQL;QACRM,WAAW;QACXC,YAAY,CAAC;IACf;IACAC,SAASX,UAAUO;IAEnB,+CAA+C;IAC/C,EAAE;IACF,yEAAyE;IACzE,yEAAyE;IACzE,2EAA2E;IAC3E,2EAA2E;IAC3E,qBAAqB;IACrBK;AACF;AAEA,SAASA;IACP,IAAIR,wBAAwB,CAACS,uBAAuB;QAClD,wEAAwE;QACxE,oEAAoE;QACpE,iEAAiE;QACjE;IACF;IACAT,uBAAuB;IACvBb,kBAAkBuB;AACpB;AAEA;;;;;CAKC,GACD,SAASD;IACP,yEAAyE;IACzE,wEAAwE;IACxE,2EAA2E;IAC3E,sBAAsB;IACtB,OAAOX,qBAAqBD;AAC9B;AAEA;;;;;CAKC,GACD,OAAO,SAASc,8BACdC,oBAAsC;IAEtCd;IACAc,qBAAqBpB,IAAI,CACvBqB,6BACAA;AAEJ;AAEA,MAAMC,OAAO,KAAO;AAEpB,OAAO,SAASC,qBAAqBC,OAAqB;IACxD,qEAAqE;IACrE,wEAAwE;IACxE,+CAA+C;IAC/C,EAAE;IACF,0EAA0E;IAC1EA,QAAQxB,IAAI,CAACsB,MAAMA;AACrB;AAEA,SAASD;IACPf;IAEA,qEAAqE;IACrE,oBAAoB;IACpBU;AACF;AAEA;;;;CAIC,GACD,OAAO,SAASS,iBAAiBd,IAAkB;IACjD,yEAAyE;IACzE,IAAI,CAACA,KAAKE,SAAS,EAAE;QACnB,8BAA8B;QAC9B;IACF;IACA,mCAAmC;IACnCF,KAAKE,SAAS,GAAG;IACjBE,SAASX,UAAUO;IACnBK;AACF;AAEA,SAASE;IACPV,uBAAuB;IAEvB,0EAA0E;IAC1E,4EAA4E;IAC5E,wDAAwD;IACxD,MAAMkB,MAAMC,KAAKD,GAAG;IAEpB,gEAAgE;IAChE,IAAIf,OAAOiB,SAASxB;IACpB,MAAOO,SAAS,QAAQM,sBAAuB;QAC7C,MAAMY,QAAQrC,gCAAgCkC,KAAKf;QACnD,MAAMmB,aAAaC,cAAcL,KAAKf,MAAMkB;QAC5C,OAAQC;YACN;gBACE,oEAAoE;gBACpE,sDAAsD;gBACtD;YACF;gBACE,iEAAiE;gBACjE,4DAA4D;gBAC5DnB,KAAKE,SAAS,GAAG;gBAEjB,4BAA4B;gBAC5BmB,QAAQ5B;gBACRO,OAAOiB,SAASxB;gBAChB;YACF;gBACE,uDAAuD;gBACvD4B,QAAQ5B;gBACRO,OAAOiB,SAASxB;gBAChB;YACF;gBAAS;oBACP,MAAM6B,mBAA0BH;oBAChC;gBACF;QACF;IACF;AACF;AAEA,SAASC,cACPL,GAAW,EACXf,IAAkB,EAClBkB,KAAsB;IAEtB,OAAQA,MAAMK,MAAM;QAClB,KAAKxC,YAAYyC,OAAO;YAAE;gBACxB,yEAAyE;gBACzE,mBAAmB;gBACnB,MAAMC,eAAeP,MAAMO,YAAY;gBACvC,IAAIA,iBAAiB,MAAM;oBACzBP,MAAMO,YAAY,GAAG,IAAIC,IAAI;wBAAC1B;qBAAK;gBACrC,OAAO;oBACLyB,aAAaE,GAAG,CAAC3B;gBACnB;gBACA;YACF;QACA,KAAKjB,YAAY6C,QAAQ;YAAE;gBACzB,6CAA6C;gBAC7C;YACF;QACA,KAAK7C,YAAY8C,SAAS;YAAE;gBAC1B,wCAAwC;gBACxC,IAAI,CAACvB,uBAAuB;oBAC1B,0DAA0D;oBAC1D;gBACF;gBACA,MAAMwB,OAAOZ,MAAMY,IAAI;gBACvBhD,6BAA6BiC,KAAKf,MAAMkB,OAAOY,KAAKC,IAAI,EAAE;gBAC1D,OAAOC,gBAAgBjB,KAAKf,MAAMkB,OAAOY;YAC3C;QACA;YAAS;gBACP,MAAMR,mBAA0BJ;gBAChC;YACF;IACF;AACF;AAEA,SAASc,gBACPjB,GAAW,EACXf,IAAkB,EAClBkB,KAA+B,EAC/BY,IAAkB;IAElB,IAAIA,KAAKG,KAAK,KAAK,MAAM;QACvB,iCAAiC;QACjC,IAAK,MAAMC,oBAAoBJ,KAAKG,KAAK,CAAE;YACzC,MAAME,YAAYL,KAAKG,KAAK,CAACC,iBAAiB;YAC9C,IAAI,CAAC5B,uBAAuB;gBAC1B,0DAA0D;gBAC1D;YACF,OAAO;gBACL,MAAM8B,YAAYD,UAAUJ,IAAI;gBAChC,MAAMM,aAAaF,UAAUG,KAAK;gBAClCxD,6BAA6BiC,KAAKf,MAAMkB,OAAOkB,WAAWC;YAC5D;YACA,MAAME,kBAAkBP,gBAAgBjB,KAAKf,MAAMkB,OAAOiB;YAC1D,IAAII,uBAAuD;gBACzD,mCAAmC;gBACnC;YACF;QACF;IACF;IACA,+DAA+D;IAC/D;AACF;AAEA,gFAAgF;AAChF,+EAA+E;AAC/E,6EAA6E;AAC7E,qEAAqE;AACrE,gFAAgF;AAEhF,SAASC,qBAAqBC,CAAe,EAAEC,CAAe;IAC5D,6EAA6E;IAC7E,wEAAwE;IACxE,UAAU;IACV,EAAE;IACF,uEAAuE;IACvE,uCAAuC;IACvC,OAAOA,EAAEzC,MAAM,GAAGwC,EAAExC,MAAM;AAC5B;AAEA,SAASG,SAASuC,IAAyB,EAAEC,IAAkB;IAC7D,MAAMC,QAAQF,KAAKG,MAAM;IACzBH,KAAKI,IAAI,CAACH;IACVA,KAAKzC,UAAU,GAAG0C;IAClBG,WAAWL,MAAMC,MAAMC;AACzB;AAEA,SAAS5B,SAAS0B,IAAyB;IACzC,OAAOA,KAAKG,MAAM,KAAK,IAAI,OAAOH,IAAI,CAAC,EAAE;AAC3C;AAEA,SAAStB,QAAQsB,IAAyB;IACxC,IAAIA,KAAKG,MAAM,KAAK,GAAG;QACrB,OAAO;IACT;IACA,MAAMG,QAAQN,IAAI,CAAC,EAAE;IACrBM,MAAM9C,UAAU,GAAG,CAAC;IACpB,MAAM+C,OAAOP,KAAKQ,GAAG;IACrB,IAAID,SAASD,OAAO;QAClBN,IAAI,CAAC,EAAE,GAAGO;QACVA,KAAK/C,UAAU,GAAG;QAClBiD,aAAaT,MAAMO,MAAM;IAC3B;IACA,OAAOD;AACT;AAEA,sEAAsE;AACtE,mBAAmB;AACnB,qEAAqE;AACrE,kCAAkC;AAClC,wBAAwB;AACxB,4CAA4C;AAC5C,uCAAuC;AACvC,oDAAoD;AACpD,0CAA0C;AAC1C,sCAAsC;AACtC,eAAe;AACf,wDAAwD;AACxD,wCAAwC;AACxC,QAAQ;AACR,MAAM;AACN,IAAI;AAEJ,SAASD,WACPL,IAAyB,EACzBC,IAAkB,EAClBS,CAAS;IAET,IAAIR,QAAQQ;IACZ,MAAOR,QAAQ,EAAG;QAChB,MAAMS,cAAc,AAACT,QAAQ,MAAO;QACpC,MAAMU,SAASZ,IAAI,CAACW,YAAY;QAChC,IAAId,qBAAqBe,QAAQX,QAAQ,GAAG;YAC1C,wCAAwC;YACxCD,IAAI,CAACW,YAAY,GAAGV;YACpBA,KAAKzC,UAAU,GAAGmD;YAClBX,IAAI,CAACE,MAAM,GAAGU;YACdA,OAAOpD,UAAU,GAAG0C;YAEpBA,QAAQS;QACV,OAAO;YACL,+BAA+B;YAC/B;QACF;IACF;AACF;AAEA,SAASF,aACPT,IAAyB,EACzBC,IAAkB,EAClBS,CAAS;IAET,IAAIR,QAAQQ;IACZ,MAAMP,SAASH,KAAKG,MAAM;IAC1B,MAAMU,aAAaV,WAAW;IAC9B,MAAOD,QAAQW,WAAY;QACzB,MAAMC,YAAY,AAACZ,CAAAA,QAAQ,CAAA,IAAK,IAAI;QACpC,MAAMa,OAAOf,IAAI,CAACc,UAAU;QAC5B,MAAME,aAAaF,YAAY;QAC/B,MAAMG,QAAQjB,IAAI,CAACgB,WAAW;QAE9B,wEAAwE;QACxE,IAAInB,qBAAqBkB,MAAMd,QAAQ,GAAG;YACxC,IAAIe,aAAab,UAAUN,qBAAqBoB,OAAOF,QAAQ,GAAG;gBAChEf,IAAI,CAACE,MAAM,GAAGe;gBACdA,MAAMzD,UAAU,GAAG0C;gBACnBF,IAAI,CAACgB,WAAW,GAAGf;gBACnBA,KAAKzC,UAAU,GAAGwD;gBAElBd,QAAQc;YACV,OAAO;gBACLhB,IAAI,CAACE,MAAM,GAAGa;gBACdA,KAAKvD,UAAU,GAAG0C;gBAClBF,IAAI,CAACc,UAAU,GAAGb;gBAClBA,KAAKzC,UAAU,GAAGsD;gBAElBZ,QAAQY;YACV;QACF,OAAO,IAAIE,aAAab,UAAUN,qBAAqBoB,OAAOhB,QAAQ,GAAG;YACvED,IAAI,CAACE,MAAM,GAAGe;YACdA,MAAMzD,UAAU,GAAG0C;YACnBF,IAAI,CAACgB,WAAW,GAAGf;YACnBA,KAAKzC,UAAU,GAAGwD;YAElBd,QAAQc;QACV,OAAO;YACL,kCAAkC;YAClC;QACF;IACF;AACF"}