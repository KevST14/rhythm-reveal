{"version":3,"sources":["../../../../src/client/components/segment-cache/scheduler.ts"],"sourcesContent":["import type { TreePrefetch } from '../../../server/app-render/collect-segment-data'\nimport {\n  requestRouteCacheEntryFromCache,\n  requestSegmentEntryFromCache,\n  EntryStatus,\n  type FulfilledRouteCacheEntry,\n  type RouteCacheEntry,\n} from './cache'\nimport type { RouteCacheKey } from './cache-key'\n\nconst scheduleMicrotask =\n  typeof queueMicrotask === 'function'\n    ? queueMicrotask\n    : (fn: () => unknown) =>\n        Promise.resolve()\n          .then(fn)\n          .catch((error) =>\n            setTimeout(() => {\n              throw error\n            })\n          )\n\nexport type PrefetchTask = {\n  key: RouteCacheKey\n\n  /**\n   * sortId is an incrementing counter\n   *\n   * Newer prefetches are prioritized over older ones, so that as new links\n   * enter the viewport, they are not starved by older links that are no\n   * longer relevant. In the future, we can add additional prioritization\n   * heuristics, like removing prefetches once a link leaves the viewport.\n   *\n   * The sortId is assigned when the prefetch is initiated, and reassigned if\n   * the same URL is prefetched again (effectively bumping it to the top of\n   * the queue).\n   *\n   * TODO: We can add additional fields here to indicate what kind of prefetch\n   * it is. For example, was it initiated by a link? Or was it an imperative\n   * call? If it was initiated by a link, we can remove it from the queue when\n   * the link leaves the viewport, but if it was an imperative call, then we\n   * should keep it in the queue until it's fulfilled.\n   *\n   * We can also add priority levels. For example, hovering over a link could\n   * increase the priority of its prefetch.\n   */\n  sortId: number\n\n  /**\n   * True if the prefetch is blocked by network data. We remove tasks from the\n   * queue once they are blocked, and add them back when they receive data.\n   *\n   * isBlocked also indicates whether the task is currently in the queue; tasks\n   * are removed from the queue when they are blocked. Use this to avoid\n   * queueing the same task multiple times.\n   */\n  isBlocked: boolean\n\n  /**\n   * The index of the task in the heap's backing array. Used to efficiently\n   * change the priority of a task by re-sifting it, which requires knowing\n   * where it is in the array. This is only used internally by the heap\n   * algorithm. The naive alternative is indexOf every time a task is queued,\n   * which has O(n) complexity.\n   */\n  _heapIndex: number\n}\n\nconst enum PrefetchTaskExitStatus {\n  /**\n   * The task yielded because there are too many requests in progress.\n   */\n  InProgress,\n\n  /**\n   * The task is blocked. It needs more data before it can proceed.\n   *\n   * Currently the only reason this happens is we're still waiting to receive a\n   * route tree from the server, because we can't start prefetching the segments\n   * until we know what to prefetch.\n   */\n  Blocked,\n\n  /**\n   * There's nothing left to prefetch.\n   */\n  Done,\n}\n\nconst taskHeap: Array<PrefetchTask> = []\n\n// This is intentionally low so that when a navigation happens, the browser's\n// internal network queue is not already saturated with prefetch requests.\nconst MAX_CONCURRENT_PREFETCH_REQUESTS = 3\nlet inProgressRequests = 0\n\nlet sortIdCounter = 0\nlet didScheduleMicrotask = false\n\n/**\n * Initiates a prefetch task for the given URL. If a prefetch for the same URL\n * is already in progress, this will bump it to the top of the queue.\n *\n * This is not a user-facing function. By the time this is called, the href is\n * expected to be validated and normalized.\n *\n * @param key The RouteCacheKey to prefetch.\n */\nexport function schedulePrefetchTask(key: RouteCacheKey): void {\n  // Spawn a new prefetch task\n  const task: PrefetchTask = {\n    key,\n    sortId: sortIdCounter++,\n    isBlocked: false,\n    _heapIndex: -1,\n  }\n  heapPush(taskHeap, task)\n\n  // Schedule an async task to process the queue.\n  //\n  // The main reason we process the queue in an async task is for batching.\n  // It's common for a single JS task/event to trigger multiple prefetches.\n  // By deferring to a microtask, we only process the queue once per JS task.\n  // If they have different priorities, it also ensures they are processed in\n  // the optimal order.\n  ensureWorkIsScheduled()\n}\n\nfunction ensureWorkIsScheduled() {\n  if (didScheduleMicrotask || !hasNetworkBandwidth()) {\n    // Either we already scheduled a task to process the queue, or there are\n    // too many concurrent requests in progress. In the latter case, the\n    // queue will resume processing once more bandwidth is available.\n    return\n  }\n  didScheduleMicrotask = true\n  scheduleMicrotask(processQueueInMicrotask)\n}\n\n/**\n * Checks if we've exceeded the maximum number of concurrent prefetch requests,\n * to avoid saturating the browser's internal network queue. This is a\n * cooperative limit â€” prefetch tasks should check this before issuing\n * new requests.\n */\nfunction hasNetworkBandwidth(): boolean {\n  // TODO: Also check if there's an in-progress navigation. We should never\n  // add prefetch requests to the network queue if an actual navigation is\n  // taking place, to ensure there's sufficient bandwidth for render-blocking\n  // data and resources.\n  return inProgressRequests < MAX_CONCURRENT_PREFETCH_REQUESTS\n}\n\n/**\n * Notifies the scheduler of an in-progress prefetch request. This is used to\n * control network bandwidth by limiting the number of concurrent requests.\n *\n * @param promise A promise that resolves when the request has finished.\n */\nexport function trackPrefetchRequestBandwidth(\n  promiseForServerData: Promise<unknown>\n) {\n  inProgressRequests++\n  promiseForServerData.then(\n    onPrefetchRequestCompletion,\n    onPrefetchRequestCompletion\n  )\n}\n\nconst noop = () => {}\n\nexport function spawnPrefetchSubtask(promise: Promise<any>) {\n  // When the scheduler spawns an async task, we don't await its result\n  // directly. Instead, the async task writes its result directly into the\n  // cache, then pings the scheduler to continue.\n  //\n  // This function only exists to prevent warnings about unhandled promises.\n  promise.then(noop, noop)\n}\n\nfunction onPrefetchRequestCompletion(): void {\n  inProgressRequests--\n\n  // Notify the scheduler that we have more bandwidth, and can continue\n  // processing tasks.\n  ensureWorkIsScheduled()\n}\n\n/**\n * Notify the scheduler that we've received new data for an in-progress\n * prefetch. The corresponding task will be added back to the queue (unless the\n * task has been canceled in the meantime).\n */\nexport function pingPrefetchTask(task: PrefetchTask) {\n  // \"Ping\" a prefetch that's already in progress to notify it of new data.\n  if (!task.isBlocked) {\n    // Prefetch is already queued.\n    return\n  }\n  // Unblock the task and requeue it.\n  task.isBlocked = false\n  heapPush(taskHeap, task)\n  ensureWorkIsScheduled()\n}\n\nfunction processQueueInMicrotask() {\n  didScheduleMicrotask = false\n\n  // We aim to minimize how often we read the current time. Since nearly all\n  // functions in the prefetch scheduler are synchronous, we can read the time\n  // once and pass it as an argument wherever it's needed.\n  const now = Date.now()\n\n  // Process the task queue until we run out of network bandwidth.\n  let task = heapPeek(taskHeap)\n  while (task !== null && hasNetworkBandwidth()) {\n    const route = requestRouteCacheEntryFromCache(now, task)\n    const exitStatus = pingRouteTree(now, task, route)\n    switch (exitStatus) {\n      case PrefetchTaskExitStatus.InProgress:\n        // The task yielded because there are too many requests in progress.\n        // Stop processing tasks until we have more bandwidth.\n        return\n      case PrefetchTaskExitStatus.Blocked:\n        // The task is blocked. It needs more data before it can proceed.\n        // Keep the task out of the queue until the server responds.\n        task.isBlocked = true\n\n        // Continue to the next task\n        heapPop(taskHeap)\n        task = heapPeek(taskHeap)\n        continue\n      case PrefetchTaskExitStatus.Done:\n        // The prefetch is complete. Continue to the next task.\n        heapPop(taskHeap)\n        task = heapPeek(taskHeap)\n        continue\n      default: {\n        const _exhaustiveCheck: never = exitStatus\n        return\n      }\n    }\n  }\n}\n\nfunction pingRouteTree(\n  now: number,\n  task: PrefetchTask,\n  route: RouteCacheEntry\n): PrefetchTaskExitStatus {\n  switch (route.status) {\n    case EntryStatus.Pending: {\n      // Still pending. We can't start prefetching the segments until the route\n      // tree has loaded.\n      const blockedTasks = route.blockedTasks\n      if (blockedTasks === null) {\n        route.blockedTasks = new Set([task])\n      } else {\n        blockedTasks.add(task)\n      }\n      return PrefetchTaskExitStatus.Blocked\n    }\n    case EntryStatus.Rejected: {\n      // Route tree failed to load. Treat as a 404.\n      return PrefetchTaskExitStatus.Done\n    }\n    case EntryStatus.Fulfilled: {\n      // Recursively fill in the segment tree.\n      if (!hasNetworkBandwidth()) {\n        // Stop prefetching segments until there's more bandwidth.\n        return PrefetchTaskExitStatus.InProgress\n      }\n      const tree = route.tree\n      requestSegmentEntryFromCache(now, task, route, tree.path, '')\n      return pingSegmentTree(now, task, route, tree)\n    }\n    default: {\n      const _exhaustiveCheck: never = route\n      return PrefetchTaskExitStatus.Done\n    }\n  }\n}\n\nfunction pingSegmentTree(\n  now: number,\n  task: PrefetchTask,\n  route: FulfilledRouteCacheEntry,\n  tree: TreePrefetch\n): PrefetchTaskExitStatus.InProgress | PrefetchTaskExitStatus.Done {\n  if (tree.slots !== null) {\n    // Recursively ping the children.\n    for (const parallelRouteKey in tree.slots) {\n      const childTree = tree.slots[parallelRouteKey]\n      if (!hasNetworkBandwidth()) {\n        // Stop prefetching segments until there's more bandwidth.\n        return PrefetchTaskExitStatus.InProgress\n      } else {\n        const childPath = childTree.path\n        const childToken = childTree.token\n        requestSegmentEntryFromCache(now, task, route, childPath, childToken)\n      }\n      const childExitStatus = pingSegmentTree(now, task, route, childTree)\n      if (childExitStatus === PrefetchTaskExitStatus.InProgress) {\n        // Child yielded without finishing.\n        return PrefetchTaskExitStatus.InProgress\n      }\n    }\n  }\n  // This segment and all its children have finished prefetching.\n  return PrefetchTaskExitStatus.Done\n}\n\n// -----------------------------------------------------------------------------\n// The remainider of the module is a MinHeap implementation. Try not to put any\n// logic below here unless it's related to the heap algorithm. We can extract\n// this to a separate module if/when we need multiple kinds of heaps.\n// -----------------------------------------------------------------------------\n\nfunction compareQueuePriority(a: PrefetchTask, b: PrefetchTask) {\n  // Since the queue is a MinHeap, this should return a positive number if b is\n  // higher priority than a, and a negative number if a is higher priority\n  // than b.\n  //\n  // sortId is an incrementing counter assigned to prefetches. We want to\n  // process the newest prefetches first.\n  return b.sortId - a.sortId\n}\n\nfunction heapPush(heap: Array<PrefetchTask>, node: PrefetchTask): void {\n  const index = heap.length\n  heap.push(node)\n  node._heapIndex = index\n  heapSiftUp(heap, node, index)\n}\n\nfunction heapPeek(heap: Array<PrefetchTask>): PrefetchTask | null {\n  return heap.length === 0 ? null : heap[0]\n}\n\nfunction heapPop(heap: Array<PrefetchTask>): PrefetchTask | null {\n  if (heap.length === 0) {\n    return null\n  }\n  const first = heap[0]\n  first._heapIndex = -1\n  const last = heap.pop() as PrefetchTask\n  if (last !== first) {\n    heap[0] = last\n    last._heapIndex = 0\n    heapSiftDown(heap, last, 0)\n  }\n  return first\n}\n\n// Not currently used, but will be once we add the ability to update a\n// task's priority.\n// function heapSift(heap: Array<PrefetchTask>, node: PrefetchTask) {\n//   const index = node._heapIndex\n//   if (index !== -1) {\n//     const parentIndex = (index - 1) >>> 1\n//     const parent = heap[parentIndex]\n//     if (compareQueuePriority(parent, node) > 0) {\n//       // The parent is larger. Sift up.\n//       heapSiftUp(heap, node, index)\n//     } else {\n//       // The parent is smaller (or equal). Sift down.\n//       heapSiftDown(heap, node, index)\n//     }\n//   }\n// }\n\nfunction heapSiftUp(\n  heap: Array<PrefetchTask>,\n  node: PrefetchTask,\n  i: number\n): void {\n  let index = i\n  while (index > 0) {\n    const parentIndex = (index - 1) >>> 1\n    const parent = heap[parentIndex]\n    if (compareQueuePriority(parent, node) > 0) {\n      // The parent is larger. Swap positions.\n      heap[parentIndex] = node\n      node._heapIndex = parentIndex\n      heap[index] = parent\n      parent._heapIndex = index\n\n      index = parentIndex\n    } else {\n      // The parent is smaller. Exit.\n      return\n    }\n  }\n}\n\nfunction heapSiftDown(\n  heap: Array<PrefetchTask>,\n  node: PrefetchTask,\n  i: number\n): void {\n  let index = i\n  const length = heap.length\n  const halfLength = length >>> 1\n  while (index < halfLength) {\n    const leftIndex = (index + 1) * 2 - 1\n    const left = heap[leftIndex]\n    const rightIndex = leftIndex + 1\n    const right = heap[rightIndex]\n\n    // If the left or right node is smaller, swap with the smaller of those.\n    if (compareQueuePriority(left, node) < 0) {\n      if (rightIndex < length && compareQueuePriority(right, left) < 0) {\n        heap[index] = right\n        right._heapIndex = index\n        heap[rightIndex] = node\n        node._heapIndex = rightIndex\n\n        index = rightIndex\n      } else {\n        heap[index] = left\n        left._heapIndex = index\n        heap[leftIndex] = node\n        node._heapIndex = leftIndex\n\n        index = leftIndex\n      }\n    } else if (rightIndex < length && compareQueuePriority(right, node) < 0) {\n      heap[index] = right\n      right._heapIndex = index\n      heap[rightIndex] = node\n      node._heapIndex = rightIndex\n\n      index = rightIndex\n    } else {\n      // Neither child is smaller. Exit.\n      return\n    }\n  }\n}\n"],"names":["pingPrefetchTask","schedulePrefetchTask","spawnPrefetchSubtask","trackPrefetchRequestBandwidth","scheduleMicrotask","queueMicrotask","fn","Promise","resolve","then","catch","error","setTimeout","taskHeap","MAX_CONCURRENT_PREFETCH_REQUESTS","inProgressRequests","sortIdCounter","didScheduleMicrotask","key","task","sortId","isBlocked","_heapIndex","heapPush","ensureWorkIsScheduled","hasNetworkBandwidth","processQueueInMicrotask","promiseForServerData","onPrefetchRequestCompletion","noop","promise","now","Date","heapPeek","route","requestRouteCacheEntryFromCache","exitStatus","pingRouteTree","heapPop","_exhaustiveCheck","status","EntryStatus","Pending","blockedTasks","Set","add","Rejected","Fulfilled","tree","requestSegmentEntryFromCache","path","pingSegmentTree","slots","parallelRouteKey","childTree","childPath","childToken","token","childExitStatus","compareQueuePriority","a","b","heap","node","index","length","push","heapSiftUp","first","last","pop","heapSiftDown","i","parentIndex","parent","halfLength","leftIndex","left","rightIndex","right"],"mappings":";;;;;;;;;;;;;;;;;IAiMgBA,gBAAgB;eAAhBA;;IArFAC,oBAAoB;eAApBA;;IA+DAC,oBAAoB;eAApBA;;IAZAC,6BAA6B;eAA7BA;;;uBAxJT;AAGP,MAAMC,oBACJ,OAAOC,mBAAmB,aACtBA,iBACA,CAACC,KACCC,QAAQC,OAAO,GACZC,IAAI,CAACH,IACLI,KAAK,CAAC,CAACC,QACNC,WAAW;YACT,MAAMD;QACR;AAsEZ,MAAME,WAAgC,EAAE;AAExC,6EAA6E;AAC7E,0EAA0E;AAC1E,MAAMC,mCAAmC;AACzC,IAAIC,qBAAqB;AAEzB,IAAIC,gBAAgB;AACpB,IAAIC,uBAAuB;AAWpB,SAAShB,qBAAqBiB,GAAkB;IACrD,4BAA4B;IAC5B,MAAMC,OAAqB;QACzBD;QACAE,QAAQJ;QACRK,WAAW;QACXC,YAAY,CAAC;IACf;IACAC,SAASV,UAAUM;IAEnB,+CAA+C;IAC/C,EAAE;IACF,yEAAyE;IACzE,yEAAyE;IACzE,2EAA2E;IAC3E,2EAA2E;IAC3E,qBAAqB;IACrBK;AACF;AAEA,SAASA;IACP,IAAIP,wBAAwB,CAACQ,uBAAuB;QAClD,wEAAwE;QACxE,oEAAoE;QACpE,iEAAiE;QACjE;IACF;IACAR,uBAAuB;IACvBb,kBAAkBsB;AACpB;AAEA;;;;;CAKC,GACD,SAASD;IACP,yEAAyE;IACzE,wEAAwE;IACxE,2EAA2E;IAC3E,sBAAsB;IACtB,OAAOV,qBAAqBD;AAC9B;AAQO,SAASX,8BACdwB,oBAAsC;IAEtCZ;IACAY,qBAAqBlB,IAAI,CACvBmB,6BACAA;AAEJ;AAEA,MAAMC,OAAO,KAAO;AAEb,SAAS3B,qBAAqB4B,OAAqB;IACxD,qEAAqE;IACrE,wEAAwE;IACxE,+CAA+C;IAC/C,EAAE;IACF,0EAA0E;IAC1EA,QAAQrB,IAAI,CAACoB,MAAMA;AACrB;AAEA,SAASD;IACPb;IAEA,qEAAqE;IACrE,oBAAoB;IACpBS;AACF;AAOO,SAASxB,iBAAiBmB,IAAkB;IACjD,yEAAyE;IACzE,IAAI,CAACA,KAAKE,SAAS,EAAE;QACnB,8BAA8B;QAC9B;IACF;IACA,mCAAmC;IACnCF,KAAKE,SAAS,GAAG;IACjBE,SAASV,UAAUM;IACnBK;AACF;AAEA,SAASE;IACPT,uBAAuB;IAEvB,0EAA0E;IAC1E,4EAA4E;IAC5E,wDAAwD;IACxD,MAAMc,MAAMC,KAAKD,GAAG;IAEpB,gEAAgE;IAChE,IAAIZ,OAAOc,SAASpB;IACpB,MAAOM,SAAS,QAAQM,sBAAuB;QAC7C,MAAMS,QAAQC,IAAAA,sCAA+B,EAACJ,KAAKZ;QACnD,MAAMiB,aAAaC,cAAcN,KAAKZ,MAAMe;QAC5C,OAAQE;YACN;gBACE,oEAAoE;gBACpE,sDAAsD;gBACtD;YACF;gBACE,iEAAiE;gBACjE,4DAA4D;gBAC5DjB,KAAKE,SAAS,GAAG;gBAEjB,4BAA4B;gBAC5BiB,QAAQzB;gBACRM,OAAOc,SAASpB;gBAChB;YACF;gBACE,uDAAuD;gBACvDyB,QAAQzB;gBACRM,OAAOc,SAASpB;gBAChB;YACF;gBAAS;oBACP,MAAM0B,mBAA0BH;oBAChC;gBACF;QACF;IACF;AACF;AAEA,SAASC,cACPN,GAAW,EACXZ,IAAkB,EAClBe,KAAsB;IAEtB,OAAQA,MAAMM,MAAM;QAClB,KAAKC,kBAAW,CAACC,OAAO;YAAE;gBACxB,yEAAyE;gBACzE,mBAAmB;gBACnB,MAAMC,eAAeT,MAAMS,YAAY;gBACvC,IAAIA,iBAAiB,MAAM;oBACzBT,MAAMS,YAAY,GAAG,IAAIC,IAAI;wBAACzB;qBAAK;gBACrC,OAAO;oBACLwB,aAAaE,GAAG,CAAC1B;gBACnB;gBACA;YACF;QACA,KAAKsB,kBAAW,CAACK,QAAQ;YAAE;gBACzB,6CAA6C;gBAC7C;YACF;QACA,KAAKL,kBAAW,CAACM,SAAS;YAAE;gBAC1B,wCAAwC;gBACxC,IAAI,CAACtB,uBAAuB;oBAC1B,0DAA0D;oBAC1D;gBACF;gBACA,MAAMuB,OAAOd,MAAMc,IAAI;gBACvBC,IAAAA,mCAA4B,EAAClB,KAAKZ,MAAMe,OAAOc,KAAKE,IAAI,EAAE;gBAC1D,OAAOC,gBAAgBpB,KAAKZ,MAAMe,OAAOc;YAC3C;QACA;YAAS;gBACP,MAAMT,mBAA0BL;gBAChC;YACF;IACF;AACF;AAEA,SAASiB,gBACPpB,GAAW,EACXZ,IAAkB,EAClBe,KAA+B,EAC/Bc,IAAkB;IAElB,IAAIA,KAAKI,KAAK,KAAK,MAAM;QACvB,iCAAiC;QACjC,IAAK,MAAMC,oBAAoBL,KAAKI,KAAK,CAAE;YACzC,MAAME,YAAYN,KAAKI,KAAK,CAACC,iBAAiB;YAC9C,IAAI,CAAC5B,uBAAuB;gBAC1B,0DAA0D;gBAC1D;YACF,OAAO;gBACL,MAAM8B,YAAYD,UAAUJ,IAAI;gBAChC,MAAMM,aAAaF,UAAUG,KAAK;gBAClCR,IAAAA,mCAA4B,EAAClB,KAAKZ,MAAMe,OAAOqB,WAAWC;YAC5D;YACA,MAAME,kBAAkBP,gBAAgBpB,KAAKZ,MAAMe,OAAOoB;YAC1D,IAAII,uBAAuD;gBACzD,mCAAmC;gBACnC;YACF;QACF;IACF;IACA,+DAA+D;IAC/D;AACF;AAEA,gFAAgF;AAChF,+EAA+E;AAC/E,6EAA6E;AAC7E,qEAAqE;AACrE,gFAAgF;AAEhF,SAASC,qBAAqBC,CAAe,EAAEC,CAAe;IAC5D,6EAA6E;IAC7E,wEAAwE;IACxE,UAAU;IACV,EAAE;IACF,uEAAuE;IACvE,uCAAuC;IACvC,OAAOA,EAAEzC,MAAM,GAAGwC,EAAExC,MAAM;AAC5B;AAEA,SAASG,SAASuC,IAAyB,EAAEC,IAAkB;IAC7D,MAAMC,QAAQF,KAAKG,MAAM;IACzBH,KAAKI,IAAI,CAACH;IACVA,KAAKzC,UAAU,GAAG0C;IAClBG,WAAWL,MAAMC,MAAMC;AACzB;AAEA,SAAS/B,SAAS6B,IAAyB;IACzC,OAAOA,KAAKG,MAAM,KAAK,IAAI,OAAOH,IAAI,CAAC,EAAE;AAC3C;AAEA,SAASxB,QAAQwB,IAAyB;IACxC,IAAIA,KAAKG,MAAM,KAAK,GAAG;QACrB,OAAO;IACT;IACA,MAAMG,QAAQN,IAAI,CAAC,EAAE;IACrBM,MAAM9C,UAAU,GAAG,CAAC;IACpB,MAAM+C,OAAOP,KAAKQ,GAAG;IACrB,IAAID,SAASD,OAAO;QAClBN,IAAI,CAAC,EAAE,GAAGO;QACVA,KAAK/C,UAAU,GAAG;QAClBiD,aAAaT,MAAMO,MAAM;IAC3B;IACA,OAAOD;AACT;AAEA,sEAAsE;AACtE,mBAAmB;AACnB,qEAAqE;AACrE,kCAAkC;AAClC,wBAAwB;AACxB,4CAA4C;AAC5C,uCAAuC;AACvC,oDAAoD;AACpD,0CAA0C;AAC1C,sCAAsC;AACtC,eAAe;AACf,wDAAwD;AACxD,wCAAwC;AACxC,QAAQ;AACR,MAAM;AACN,IAAI;AAEJ,SAASD,WACPL,IAAyB,EACzBC,IAAkB,EAClBS,CAAS;IAET,IAAIR,QAAQQ;IACZ,MAAOR,QAAQ,EAAG;QAChB,MAAMS,cAAc,AAACT,QAAQ,MAAO;QACpC,MAAMU,SAASZ,IAAI,CAACW,YAAY;QAChC,IAAId,qBAAqBe,QAAQX,QAAQ,GAAG;YAC1C,wCAAwC;YACxCD,IAAI,CAACW,YAAY,GAAGV;YACpBA,KAAKzC,UAAU,GAAGmD;YAClBX,IAAI,CAACE,MAAM,GAAGU;YACdA,OAAOpD,UAAU,GAAG0C;YAEpBA,QAAQS;QACV,OAAO;YACL,+BAA+B;YAC/B;QACF;IACF;AACF;AAEA,SAASF,aACPT,IAAyB,EACzBC,IAAkB,EAClBS,CAAS;IAET,IAAIR,QAAQQ;IACZ,MAAMP,SAASH,KAAKG,MAAM;IAC1B,MAAMU,aAAaV,WAAW;IAC9B,MAAOD,QAAQW,WAAY;QACzB,MAAMC,YAAY,AAACZ,CAAAA,QAAQ,CAAA,IAAK,IAAI;QACpC,MAAMa,OAAOf,IAAI,CAACc,UAAU;QAC5B,MAAME,aAAaF,YAAY;QAC/B,MAAMG,QAAQjB,IAAI,CAACgB,WAAW;QAE9B,wEAAwE;QACxE,IAAInB,qBAAqBkB,MAAMd,QAAQ,GAAG;YACxC,IAAIe,aAAab,UAAUN,qBAAqBoB,OAAOF,QAAQ,GAAG;gBAChEf,IAAI,CAACE,MAAM,GAAGe;gBACdA,MAAMzD,UAAU,GAAG0C;gBACnBF,IAAI,CAACgB,WAAW,GAAGf;gBACnBA,KAAKzC,UAAU,GAAGwD;gBAElBd,QAAQc;YACV,OAAO;gBACLhB,IAAI,CAACE,MAAM,GAAGa;gBACdA,KAAKvD,UAAU,GAAG0C;gBAClBF,IAAI,CAACc,UAAU,GAAGb;gBAClBA,KAAKzC,UAAU,GAAGsD;gBAElBZ,QAAQY;YACV;QACF,OAAO,IAAIE,aAAab,UAAUN,qBAAqBoB,OAAOhB,QAAQ,GAAG;YACvED,IAAI,CAACE,MAAM,GAAGe;YACdA,MAAMzD,UAAU,GAAG0C;YACnBF,IAAI,CAACgB,WAAW,GAAGf;YACnBA,KAAKzC,UAAU,GAAGwD;YAElBd,QAAQc;QACV,OAAO;YACL,kCAAkC;YAClC;QACF;IACF;AACF"}